{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By Peter A. Stokes, École Pratique des Hautes Études – Université PSL**\n",
    "\n",
    "_These are brief notes and exercises on working with TEI XML files using Python. They are intended as teaching aids for the course on 'Image Processing with Python' which is part of the Atelier de formation annuel du Consortium Cahier on the topic of 'Exploiter les corpus d'auteurs' in Poitiers, 18--20 June 2019. For more details see https://cahier.hypotheses.org/4662_\n",
    "\n",
    "These notes assume a good knowledge of TEI XML but assume **no experience or knowledge at all** in programming. This notebook also assumes that [NumPy](https://www.numpy.org), [SciPy Lib](https://www.scipy.org/scipylib/index.html) and [Scikit-Image](https://scikit-image.org) has already been installed in your Python system.\n",
    "\n",
    "_If you are viewing this in Jupyter then you can edit the code simply by typing in the boxes. You can also execute the code in any box by clicking on the box and typing SHIFT + ENTER or using the 'Run' button in the menubar above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is significantly more advanced than the one using PIL. It shows a (rough) way of counting the number of lines of text in the image of a  page. It uses a fairly simple technique and only really works for very clean manuscripts with pretty regular lines of text.\n",
    "\n",
    "In order to do this, we need to use more complex processes than are available in PIL. Instead, we will use very well-established libraries for numerical and data processing, NumPy and SciKit-Image.\n",
    "\n",
    "First, we import our libraries and set up our variables as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters, segmentation\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.color import rgb2grey\n",
    "from scipy.signal import argrelmax, argrelmin, savgol_filter\n",
    "\n",
    "# Create an empty list variable for use later\n",
    "lines_per_page = []\n",
    "\n",
    "# Save the path to the image file\n",
    "f = \"../Example Texts/Montaignef22_25pct.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "Almost always when we work with images, we need to go through some pre-processing. This usually involves things like turning colour images into black and white, potentially clearing out any 'noise', and so on.\n",
    "\n",
    "The first thing we want to do here is threshold the image. This means that we turn the image into a simple black and white, where dark sections (i.e. ink) are white, and light sections (i.e. background) are black. The method we use here is reproduced from 'Simple Image Segmentation with Scikit-Image': http://douglasduhaime.com/posts/simple-image-segmentation-with-scikit-image.html The details of how it works are a bit complicated, so don't worry if you don't understand it fully. In summary, though, it does the following:\n",
    "\n",
    "1. Convert the image from colour to gray (since we don't need colour anymore).\n",
    "1. Calculate the threshold level of how dark something needs to be to colour it white, and vice versa. To do this we use a built-in function.\n",
    "1. Create a 'mask', that is, a map of all the pixels with value `True` if the pixel value is below the threshold, and `False` if it is above the threshold.\n",
    "1. Use the built-in `segmentation.clear_border` function to remove any `True` pixels at the border of the image, as these are almost certainly just rubbish.\n",
    "\n",
    "**You may receive a warning when you run this code**. If so then just ignore it: it's not our problem, and the code will still work.\n",
    "\n",
    "This may also take some time, depending on the size of your image and the speed of your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_rgb = imread(f)\n",
    "im = rgb2grey(im_rgb)\n",
    "\n",
    "print('Grayscale image:')\n",
    "imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we find a dividing line between 0 and 255. Pixels below this value\n",
    "# will be black, and pixels above this value will be white.\n",
    "\n",
    "val = filters.threshold_otsu(im)\n",
    "\n",
    "# The mask object converts each pixel in the image to True or False\n",
    "# to indicate whether the given pixel is black or white\n",
    "mask = im < val\n",
    "\n",
    "# Now we apply the mask to the image object\n",
    "im_final = segmentation.clear_border(mask)\n",
    "\n",
    "print('Masked image:')\n",
    "imshow(im_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Rows and Columns of Text\n",
    "\n",
    "Now, finally, we have a nice binarised image, and so we can start our analysis. Specifically, we want to find the rows and columns of text. This is a surprisingly difficult job for a computer, and there are many very sophisticated methods around. We will use a very simple one here, but one that does still work at least in very easy cases.\n",
    "\n",
    "To do this, first we want to add up all the pixels in each row and save it, and do the same per column. Fortunately this is very easy with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vals = im_final.sum(axis=1)\n",
    "col_vals = im_final.sum(axis=0)\n",
    "\n",
    "# Show the outputs\n",
    "print('Column values')\n",
    "plt.plot(col_vals)\n",
    "plt.show()\n",
    "\n",
    "print('Row values')\n",
    "plt.plot(row_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are nice, but they're very noisy: there are lots of 'ups and downs'. It would be better if we can smooth out the lines a bit. Let's use a fancy function called a 'Savitzky-Golay filter'. Frankly I don't fully understand how it works myself, but it doesn't really matter: that's the beauty of using libraries that other people have created.\n",
    "\n",
    "The only hard part here is that the Savitzky-Golay filter needs two parameters, and it's difficult to figure out what they should be, partly because they depend on the size of your image. The first number must be an odd number; experimenting suggests to me that it should be about 1/30 of the total height. The second number seems to work with '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_length = int(len(row_vals) / 30)\n",
    "\n",
    "# Remember that the window length must be an odd number\n",
    "# The % is the 'modulo' or 'remainder' operator: here it gives\n",
    "# the remainder if win_length is divided by two.\n",
    "# The += operator means 'add this to the current value of the variable'\n",
    "\n",
    "if win_length % 2 == 0:\n",
    "    win_length += 1\n",
    "\n",
    "print(win_length)\n",
    "\n",
    "\n",
    "smoothed = savgol_filter(row_vals, win_length, 3)\n",
    "plt.plot(smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look carefully at the results, you will see that it comprises a number of peaks. Each peak here corresponds to a row of text, with the value of the axis giving the y-coordinate of the line of text. This means that to find the lines, we want to find the peaks in the row values. This again is fairly easy to do with the SciKit signal processing library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, = argrelmax(smoothed, order=10)  # NOTE THE COMMA AFTER 'peaks'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some of these are 'false' peaks, namely only small peaks caused by other things on the page. Let's count only those peaks which are greater than a particular value: we can try only those peaks that are at least one third of the highest peak. You may need to change this depending on your image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_peak_height = smoothed.max() / 3\n",
    "\n",
    "are_true_peaks = smoothed[peaks] > min_peak_height\n",
    "row_peaks = peaks[are_true_peaks]\n",
    "\n",
    "print('Your script has found', len(row_peaks), 'lines of text in your image.')\n",
    "print('The y-coordinates of the lines of text are', row_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a small problem here: the system has identified the top and bottom edges of the page as lines of text. (Can you see why? Hint: look closely at the results of the segmentation image above.) This is no problem, though: we can use list slicing to remove the first and last element of the list. The code to do this is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_peaks = row_peaks[1:-1]\n",
    "print('The y-coordinates of the lines of text are', row_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Text Column\n",
    "\n",
    "Now we need to detect the coordinates of the column. The process is similar, in that we smooth the `col_values` and then look for certain results. Let's start by smoothing the signal as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_length = int(len(col_vals) / 30)\n",
    "\n",
    "# Remember that the window length must be an odd number\n",
    "# The % is the 'modulo' or 'remainder' operator: here it gives\n",
    "# the remainder if win_length is divided by two.\n",
    "# The += operator means 'add this to the current value of the variable'\n",
    "\n",
    "if win_length % 2 == 0:\n",
    "    win_length += 1\n",
    "\n",
    "col_smoothed = savgol_filter(col_vals, win_length, 3)\n",
    "plt.plot(col_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look carefully at the result. You will see that there is a big wide section in the middle which corresponds to the column of text. We need to find the start and end of this wide peak. Notice, also, that the value is very low just before the big jump to the wide section. This suggests that the easiest way is to look for the _minimum_ value rather than the _maximum_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peaks, = argrelmin(col_smoothed, order=10)  # NOTE THE COMMA AFTER 'peaks'!\n",
    "\n",
    "are_true_peaks = col_smoothed[peaks] < 0\n",
    "col_peaks = peaks[are_true_peaks]\n",
    "\n",
    "print(col_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the column and rows, we want to convert them into the `start_x`, `start_y` etc. that we need for our IIIF code from Worksheet 4. Most of this is very easy: the only slightly complicated bit is finding the height of each line. This can vary slightly, so in order to get the best results let's find the average height and go from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the line height, let's calculate the average difference between lines\n",
    "line_heights = []\n",
    "for i in range(len(row_peaks)-1):\n",
    "    h = row_peaks[i+1] - row_peaks[i]\n",
    "    line_heights.append(h)\n",
    "\n",
    "line_height = np.mean(line_heights)\n",
    "\n",
    "print(\"Average line height is\", line_height, \"pixels\")\n",
    "\n",
    "start_x = col_peaks[0]\n",
    "col_width = col_peaks[1] - start_x\n",
    "# NB that the values here measure from the *middle* of each line,\n",
    "# so for the *top* of the line we have to subtract half the line height\n",
    "start_y = row_peaks[0] - (line_height / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it Together\n",
    "\n",
    "We can put all of this together into a single process that reads in the image, finds the text block and the lines, and calculates the different coordinates of the text, column, lines etc. In order to do this more efficiently we can use _functions_. This means that we can define a set of instructions and re-use them later, rather than typing out the same thing again and again. In this case, we have called the function `process()`, and it takes one parameter, namely the filename `f`. To use the function after we have defined it, we simply store the filename in a variable (e.g. `filename`) and then tell Python to `process(filename)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(f):\n",
    "    # Threshold and mask the image. The code here is reproduced from \n",
    "    # 'Simple Image Segmentation with Scikit-Image': \n",
    "    # http://douglasduhaime.com/posts/simple-image-segmentation-with-scikit-image.html\n",
    "    im = rgb2grey(imread(f))\n",
    "\n",
    "    # find a dividing line between 0 and 255\n",
    "    # pixels below this value will be black\n",
    "    # pixels above this value will be white\n",
    "    val = filters.threshold_otsu(im)\n",
    "    \n",
    "    # the mask object converts each pixel in the image to True or False\n",
    "    # to indicate whether the given pixel is black/white\n",
    "    mask = im < val\n",
    "\n",
    "    # Remove any border noise\n",
    "    imfinal = segmentation.clear_border(mask)\n",
    "\n",
    "    row_vals = imfinal.sum(axis=1)\n",
    "    col_vals = imfinal.sum(axis=0)\n",
    "    \n",
    "    # About 1/30 of the total seems to work for the window length.\n",
    "    # Remember that the win length must be odd\n",
    "    win_row_length = int(len(row_vals) / 30)\n",
    "    if win_row_length % 2 == 0:\n",
    "        win_row_length += 1\n",
    "\n",
    "    win_col_length = int(len(col_vals) / 30)\n",
    "    if win_col_length % 2 == 0:\n",
    "        win_col_length += 1\n",
    "\n",
    "    row_smoothed = savgol_filter(row_vals, win_row_length, 3)\n",
    "    col_smoothed = savgol_filter(col_vals, win_col_length, 3)\n",
    "    \n",
    "    # TODO: need a way of calculating the order parameters\n",
    "    row_peaks, = argrelmax(row_smoothed, order=10)\n",
    "    col_peaks, = argrelmin(col_smoothed, order=10)\n",
    "    \n",
    "    min_row_peak_height = row_smoothed.max() / 3\n",
    "\n",
    "    are_true_row_peaks = row_smoothed[row_peaks] > min_row_peak_height\n",
    "    row_peaks = row_peaks[are_true_row_peaks]\n",
    "    row_peaks = row_peaks[1:-1]\n",
    "    \n",
    "    are_true_col_peaks = col_smoothed[col_peaks] < 0\n",
    "    col_peaks = col_peaks[are_true_col_peaks]\n",
    "    \n",
    "    lines_per_page = len(row_peaks)\n",
    "        \n",
    "    line_heights = []\n",
    "    for i in range(lines_per_page - 1):\n",
    "        h = row_peaks[i+1] - row_peaks[i]\n",
    "        line_heights.append(h)\n",
    "\n",
    "    line_height = np.mean(line_heights)\n",
    "    start_x = col_peaks[0]\n",
    "    start_y = row_peaks[0] - (line_height / 2)\n",
    "    col_width = col_peaks[1] - start_x\n",
    "\n",
    "    \n",
    "    return (start_x, start_y, col_width, line_height, lines_per_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it in a function, it's very easy to use and reuse: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(start_x, start_y, col_width, line_height, lines_per_page) = process(f)\n",
    "\n",
    "print('Results for image', f)\n",
    "print('\\tLines per page\\t\\t', lines_per_page)\n",
    "print('\\tText-block start (x,y)\\t', start_x, start_y)\n",
    "print('\\tColumn width (px)\\t', col_width)\n",
    "print('\\tLine height (px)\\t', line_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Steps\n",
    "\n",
    "Look very closely at the last function and its results, and see if you can understand it all. In particular, pay attention to how the results of the function are passed back out to the rest of the code. This is definitely more advanced and may be too much for now. However, if this does make sense to you then it opens up some very interesting possibilities. From here you could easily do the following:\n",
    "\n",
    "* Write software which takes a TEI XML document, marked up in the documentary view, with a URL to the IIIF manifest in the header. From there it could:\n",
    "  * Do a search of the contents of the TEI\n",
    "  * Automatically download the image(s) of matching page(s) from the IIIF server\n",
    "  * Automatically detect the lines of text on the page(s)\n",
    "  * Find the coordinates of all lines of text containing the word\n",
    "  * Display the image(s) of the page(s), with boxes drawn around the corresponding lines of text\n",
    "  * And/or, display the lines of text alongside the images of those lines, like we saw in [Models of Authority](https://www.modelsofauthority.ac.uk/digipal/search/facets/?text_type=Transcription&page=1&img_is_public=1&locus=face&result_type=clauses&view=images).\n",
    "  \n",
    "There are a few other things that could be done here:\n",
    "* First, the function above is very 'monolithic', meaning that it does everything and is a bit repetetive. It would be much better to break it up into different functions.\n",
    "* The style of programming here isn't very good, in that it's a bit clumsy and does not use more advanced Python features such as list comprehension. You could easily rewrite it to be much more elegant as you learn more Python.\n",
    "* The system for detecting lines and columns of text is very simplistic. It works alright for simple printed books like the one we've been using, but it fails very quickly when it comes to more complex or irregular cases. There are _much_ more advanced methods out there which you can find very easily if you look around on the internet. You could easily improve the methods here by implementing some of these more advanced techniques.\n",
    "* There are, of course, many other possibilities here, depending on your imagination!\n",
    "\n",
    "If these last steps are a bit too much then you really shouldn't worry. It's meant to give you a taste of what is possible, and the fact that you have got this far after less than 20 hours of Python is a good reason to celebrate!\n",
    "\n",
    "So, most of all, go, play with Python, and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "![Licence Creative Commons](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "This work (the contents of this Jupyter Python notebook) is licenced under a [Creative Commons Attribution 4.0 International](http://creativecommons.org/licenses/by/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
